<!doctype html>
<html lang="id">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover" />
<title>Snapgram — Native Camera → 1080x1920 (with audio)</title>
<link rel="icon" href="data:,">
<style>
  :root{--btn-size:84px;}
  html,body{height:100%;margin:0;background:#000;font-family:Arial,Helvetica,sans-serif}
  /* preview canvas that fills screen (cover-like) */
  #preview { position:fixed; inset:0; width:100vw; height:100vh; display:block; background:#000; touch-action:none; z-index:1; }
  /* keep <video> present but out of screen (important for iOS) */
  video#sourceVideo {
    position:fixed; left:-9999px; top:-9999px; width:1px; height:1px; opacity:0;
  }
  #captureBtn{
    position:fixed; bottom:28px; left:50%; transform:translateX(-50%);
    width:var(--btn-size); height:var(--btn-size); border-radius:50%;
    background:rgba(255,255,255,0.95); border:6px solid rgba(255,255,255,0.7);
    z-index:50; box-shadow:0 6px 18px rgba(0,0,0,0.4); -webkit-tap-highlight-color: transparent;
  }
  #flipBtn{position:fixed; top:22px; right:18px; z-index:60; padding:8px 12px; border-radius:20px; background:rgba(0,0,0,0.45); color:#fff; border:1px solid rgba(255,255,255,0.15); font-weight:600;}
  #progressOverlay { position:fixed; left:0; right:0; bottom:140px; text-align:center; color:#fff; z-index:60; display:none; }
  #progressOverlay .box { background: rgba(0,0,0,0.65); display:inline-block; padding:10px 14px; border-radius:10px; font-size:14px; }
</style>
</head>
<body>

<!-- Hidden source video (must exist in DOM for some browsers to allow camera) -->
<video id="sourceVideo" autoplay playsinline muted></video>

<!-- Visible preview canvas -->
<canvas id="preview"></canvas>

<!-- Frame image (1080x1920). Place your frame file here with this name -->
<img id="frameImg" src="frame.png" alt="frame" style="display:none;" />

<button id="captureBtn" aria-label="Capture"></button>
<button id="flipBtn">Flip</button>

<div id="progressOverlay"><div class="box" id="progressText">Loading...</div></div>

<!-- FFmpeg (UMD build) -->
<script src="https://unpkg.com/@ffmpeg/ffmpeg@0.12.2/dist/umd/ffmpeg.min.js"></script>


<script>
/*
  Final script:
  - Mode B: use native camera resolution for input, but output forced to 1080x1920
  - Photo: 1080x1920 + frame overlay
  - Video: canvas (1080x1920)+mic -> MediaRecorder -> webm -> ffmpeg.wasm -> mp4
*/

const OUT_W = 1080, OUT_H = 1920;

const sourceVideo = document.getElementById('sourceVideo');
const preview = document.getElementById('preview');
const frameImg = document.getElementById('frameImg');
const captureBtn = document.getElementById('captureBtn');
const flipBtn = document.getElementById('flipBtn');
const progressOverlay = document.getElementById('progressOverlay');
const progressText = document.getElementById('progressText');

let usingFront = false;
let stream = null;
let micStream = null;
let cameraReady = false;
let animReq = null;

// processing canvas (fixed output resolution)
const procCanvas = document.createElement('canvas');
procCanvas.width = OUT_W;
procCanvas.height = OUT_H;
const procCtx = procCanvas.getContext('2d');

// preview canvas size matches viewport
function fitPreview(){ preview.width = window.innerWidth; preview.height = window.innerHeight; }
fitPreview(); window.addEventListener('resize', fitPreview);
const pctx = preview.getContext('2d');

// ---------- start camera (native resolution request) ----------
async function startCamera(){
  cameraReady = false;
  if (stream) { stream.getTracks().forEach(t=>t.stop()); stream=null; }
  if (micStream) { micStream.getTracks().forEach(t=>t.stop()); micStream=null; }

  try {
    stream = await navigator.mediaDevices.getUserMedia({
      video: {
        facingMode: usingFront ? 'user' : 'environment',
        // request high res native; browser will pick best supported
        width: { ideal: 4000 },
        height: { ideal: 4000 }
      },
      audio: false
    });
    sourceVideo.srcObject = stream;

    await new Promise(r => {
      if (sourceVideo.readyState >= 2) r(); else sourceVideo.onloadedmetadata = r;
    });

    cameraReady = true;
    startPreviewLoop();
  } catch (err) {
    console.error('startCamera error', err);
    alert('Gagal mengakses kamera: ' + (err && err.message ? err.message : err));
  }
}

// ---------- draw camera->procCanvas (force crop center to 9:16) ----------
function drawOnceToProc(){
  if (!cameraReady) return;

  const vW = sourceVideo.videoWidth || OUT_W;
  const vH = sourceVideo.videoHeight || OUT_H;
  const vRatio = vW / vH;
  const tRatio = OUT_W / OUT_H;

  let sx=0, sy=0, sw=vW, sh=vH;

  if (vRatio > tRatio) {
    // video wider -> crop left/right
    sw = vH * tRatio;
    sx = Math.round((vW - sw) / 2);
    sy = 0;
  } else {
    // video taller -> crop top/bottom
    sh = vW / tRatio;
    sx = 0;
    sy = Math.round((vH - sh) / 2);
  }

  procCtx.clearRect(0,0,OUT_W,OUT_H);
  try {
    procCtx.drawImage(sourceVideo, sx, sy, sw, sh, 0, 0, OUT_W, OUT_H);
  } catch(e){
    // sometimes drawImage can throw if video not ready
    // just skip this frame
    console.warn('drawImage error', e);
  }

  if (frameImg.complete) procCtx.drawImage(frameImg, 0, 0, OUT_W, OUT_H);

  // draw scaled procCanvas -> preview canvas with cover-like behavior
  const pvW = preview.width, pvH = preview.height;
  const pvRatio = pvW / pvH;
  const procRatio = OUT_W / OUT_H;

  let srcX=0, srcY=0, srcW=OUT_W, srcH=OUT_H;
  if (procRatio > pvRatio) {
    srcW = Math.round(OUT_H * pvRatio);
    srcX = Math.round((OUT_W - srcW) / 2);
  } else {
    srcH = Math.round(OUT_W / pvRatio);
    srcY = Math.round((OUT_H - srcH) / 2);
  }

  pctx.clearRect(0,0,pvW,pvH);
  pctx.drawImage(procCanvas, srcX, srcY, srcW, srcH, 0, 0, pvW, pvH);
}

// preview loop
function startPreviewLoop(){
  if (animReq) return;
  function loop(){
    drawOnceToProc();
    animReq = requestAnimationFrame(loop);
  }
  loop();
}

// ---------- Photo ----------
function downloadDataURL(url, name){
  const a = document.createElement('a'); a.href = url; a.download = name; a.click();
}
function takePhoto(){
  if (!cameraReady) return;
  drawOnceToProc();
  const dataUrl = procCanvas.toDataURL('image/jpeg', 0.92);
  downloadDataURL(dataUrl, 'photo_1080x1920.jpg');
}

// ---------- Recording with audio (MediaRecorder -> webm -> ffmpeg.wasm -> mp4) ----------
let mediaRecorder = null;
let recordedChunks = [];

// FFmpeg globals (from UMD script)
const { createFFmpeg, fetchFile } = FFmpeg;
const ffmpeg = createFFmpeg({ log: true });

async function ensureFFmpegLoaded(){
  if (!ffmpeg.isLoaded()){
    progressOverlay.style.display = 'block';
    progressText.textContent = 'Memuat FFmpeg, tunggu sebentar...';
    try {
      await ffmpeg.load();
    } catch (e) {
      console.error('ffmpeg.load failed', e);
      alert('Gagal memuat FFmpeg: ' + e.message);
      throw e;
    } finally {
      progressOverlay.style.display = 'none';
    }
  }
}

async function startRecording(){
  if (!cameraReady || mediaRecorder) return;

  try {
    // ask for mic
    micStream = await navigator.mediaDevices.getUserMedia({ audio: true, video: false });

    // get canvas stream (video frames from procCanvas)
    const canvasStream = procCanvas.captureStream(25); // target fps
    const combined = new MediaStream();
    canvasStream.getVideoTracks().forEach(t=>combined.addTrack(t));
    micStream.getAudioTracks().forEach(t=>combined.addTrack(t));

    // create media recorder in webm (vp8/opus)
    const options = { mimeType: 'video/webm;codecs=vp8,opus' };
    if (!MediaRecorder.isTypeSupported(options.mimeType)){
      // fallback
      console.warn('Preferred mimeType not supported, trying default');
      delete options.mimeType;
    }

    mediaRecorder = new MediaRecorder(combined, options);
    recordedChunks = [];

    mediaRecorder.ondataavailable = e => { if (e.data && e.data.size) recordedChunks.push(e.data); };

    mediaRecorder.onstart = () => {
      progressOverlay.style.display = 'block';
      progressText.textContent = 'Merekam...';
    };

    mediaRecorder.onstop = async () => {
      progressText.textContent = 'Mengonversi ke MP4...';
      const webmBlob = new Blob(recordedChunks, { type: 'video/webm' });

      try {
        await ensureFFmpegLoaded();

        ffmpeg.FS('writeFile', 'input.webm', await fetchFile(webmBlob));

        // convert to mp4 h264+aac
        await ffmpeg.run(
          '-i','input.webm',
          '-c:v','libx264','-preset','veryfast',
          '-c:a','aac','-b:a','128k',
          '-movflags','faststart',
          'output.mp4'
        );

        const data = ffmpeg.FS('readFile','output.mp4');
        const mp4Blob = new Blob([data.buffer], { type: 'video/mp4' });
        const url = URL.createObjectURL(mp4Blob);
        downloadDataURL(url, 'video_1080x1920_with_audio.mp4');

        // cleanup FFmpeg FS
        try{ ffmpeg.FS('unlink','input.webm'); ffmpeg.FS('unlink','output.mp4'); }catch(e){}
      } catch (e) {
        console.error('Conversion failed', e);
        alert('Gagal mengonversi video: ' + (e && e.message ? e.message : e));
      } finally {
        progressOverlay.style.display = 'none';
      }

      // cleanup tracks
      mediaRecorder = null;
      if (micStream){ micStream.getTracks().forEach(t=>t.stop()); micStream=null; }
    };

    mediaRecorder.start();
  } catch (err) {
    console.error('startRecording failed', err);
    alert('Gagal memulai perekaman: ' + (err && err.message ? err.message : err));
    progressOverlay.style.display = 'none';
    if (micStream){ micStream.getTracks().forEach(t=>t.stop()); micStream=null; }
    mediaRecorder = null;
  }
}

function stopRecording(){
  if (!mediaRecorder) return;
  try { mediaRecorder.stop(); } catch(e){ console.warn(e); }
}

// ---------- button interactions (tap vs hold) ----------
let pressTimer = null;
let holding = false;
const HOLD_DELAY = 350;

function pointerDown(e){
  e.preventDefault();
  holding = false;
  pressTimer = setTimeout(async () => {
    holding = true;
    await startRecording();
  }, HOLD_DELAY);
}
function pointerUp(e){
  e.preventDefault();
  if (pressTimer){ clearTimeout(pressTimer); pressTimer = null; }
  if (!holding) takePhoto(); else stopRecording();
}

captureBtn.addEventListener('mousedown', pointerDown);
captureBtn.addEventListener('mouseup', pointerUp);
captureBtn.addEventListener('mouseleave', ()=>{ if (pressTimer) clearTimeout(pressTimer); });
captureBtn.addEventListener('touchstart', pointerDown, {passive:false});
captureBtn.addEventListener('touchend', pointerUp);

// flip camera
flipBtn.addEventListener('click', () => {
  usingFront = !usingFront;
  startCamera();
});

// start camera on load
startCamera();

</script>
</body>
</html>

